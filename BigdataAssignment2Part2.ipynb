{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217a0445-f9aa-4d52-b24d-38061357f121",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d780c20-36c8-4e70-ad45-e3bb5d7541a5/lib/python3.9/site-packages (3.8.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d780c20-36c8-4e70-ad45-e3bb5d7541a5/lib/python3.9/site-packages (from nltk) (4.66.2)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6d780c20-36c8-4e70-ad45-e3bb5d7541a5/lib/python3.9/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.1.1)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1024e1d9-0332-472f-a79d-e90bb5bd63f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package words to /root/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stpwords =  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f438b12a-13a8-4a8f-a0b3-d17ae8c65dc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[86]: [('0',\n  'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '),\n ('0',\n  'Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  '),\n ('0',\n  'Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  '),\n ('0', 'Very little music or anything to speak of.  '),\n ('1',\n  'The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  '),\n ('0',\n  \"The rest of the movie lacks art, charm, meaning... If it's about emptiness, it works I guess because it's empty.  \"),\n ('0', 'Wasted two hours.  '),\n ('1',\n  'Saw the movie today and thought it was a good effort, good messages for kids.  '),\n ('0', 'A bit predictable.  '),\n ('1', 'Loved the casting of Jimmy Buffet as the science teacher.  ')]"
     ]
    }
   ],
   "source": [
    "raw_data=sc.textFile(\"dbfs:/FileStore/shared_uploads/sxs230164@utdallas.edu/imdb_labelled.txt\")\n",
    "data = raw_data.map(lambda x: (x.split(\"\\t\")[1],x.split(\"\\t\")[0]))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4efb778-cd8f-4ab7-b226-40ad3f0e7b18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def stopword_removal(word):\n",
    "    nostopwords = []\n",
    "    for i in word:\n",
    "        if i.lower() not in stpwords:\n",
    "            nostopwords.append(i)\n",
    "    return nostopwords\n",
    " \n",
    "def punctuation_removal(word):\n",
    "    pset = '''!()[]{};:\"\\,<>./?@#$%^&*_~'''\n",
    "    for i in word:\n",
    "        if i in pset:\n",
    "            word = word.replace(i, \"\")\n",
    "    return word\n",
    " \n",
    "def lemmatize(words):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    lem_tokens = [lemm.lemmatize(x) for x in words]\n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cbc150-70c8-4881-808f-4a9cff4bd39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[88]: [(['slow',\n   'moving',\n   'aimless',\n   'movie',\n   'distressed',\n   'drifting',\n   'young',\n   'man'],\n  '0'),\n (['sure',\n   'lost',\n   'flat',\n   'character',\n   'audience',\n   'nearly',\n   'half',\n   'walked'],\n  '0'),\n (['attempting',\n   'artiness',\n   'black',\n   'white',\n   'clever',\n   'camera',\n   'angle',\n   'movie',\n   'disappointed',\n   'became',\n   'even',\n   'ridiculous',\n   'acting',\n   'poor',\n   'plot',\n   'line',\n   'almost',\n   'non',\n   'existent'],\n  '0'),\n (['little', 'music', 'anything', 'speak'], '0'),\n (['best',\n   'scene',\n   'movie',\n   'gerardo',\n   'trying',\n   'find',\n   'song',\n   'keep',\n   'running',\n   'head'],\n  '1'),\n (['rest',\n   'movie',\n   'lack',\n   'art',\n   'charm',\n   'meaning',\n   'emptiness',\n   'work',\n   'guess',\n   'empty'],\n  '0'),\n (['wasted', 'two', 'hour'], '0'),\n (['saw',\n   'movie',\n   'today',\n   'thought',\n   'good',\n   'effort',\n   'good',\n   'message',\n   'kid'],\n  '1'),\n (['bit', 'predictable'], '0'),\n (['loved', 'casting', 'jimmy', 'buffet', 'science', 'teacher'], '1')]"
     ]
    }
   ],
   "source": [
    "cleandata1 = data.mapValues(lambda x: punctuation_removal(x))\n",
    "cleandata2 = cleandata1.mapValues(lambda x: re.sub('[^a-zA-Z]',' ',x))\n",
    "cleandata3 = cleandata2.mapValues(lambda x: x.lower()).mapValues(lambda x:nltk.word_tokenize(x))\n",
    "cleandata4 = cleandata3.mapValues(lambda x: stopword_removal(x))\n",
    "finaldata = cleandata4.mapValues(lambda x: lemmatize(x)).map(lambda x:(x[1],x[0]))\n",
    "finaldata.take(10)\n",
    "#cleandata1 removes all punctuations from data\n",
    "#cleandata2 removes all digits from cleandata1 using sub method\n",
    "#cleandata3 coverts cleandata2 into lower case and performs word tokenization\n",
    "#cleadata4 removes all stop words from word tokenization data(cleandata3)\n",
    "#finaldata lemmatizes the list of review words and finally the data is reversed as (list of words, label ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6f647b-d6c7-471d-983b-faf746a6f2ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = finaldata.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbebcd1-e7da-4286-af5a-a7775ce60364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_count = train.count()\n",
    "test_count = test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f356b5a7-8a1d-4719-be77-15a0e0c5306a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[91]: -0.6784410331702498"
     ]
    }
   ],
   "source": [
    "positives = train.filter(lambda x: x[1] == \"1\").count()\n",
    "positivecount = train.filter(lambda x: x[1] == \"1\").flatMap(lambda x: x[0]).count()\n",
    "\n",
    "positive_log_probablity = math.log(positives/ (train_count)) #used while testing phase\n",
    "positive_log_probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ecc1ec-28fd-4027-b96f-7dd1d86c68f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "negatives = train.filter(lambda x: x[1] == \"0\").count()\n",
    "negativecount = train.filter(lambda x: x[1] == \"0\").flatMap(lambda x: x[0]).count()\n",
    "# negativecount\n",
    "negative_log_probablity = math.log(negatives/ (train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a62615-10d9-4703-bc02-fa37d2822efd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "differentwords = train.flatMap(lambda x: x[0]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978790b8-88a3-4fa7-accd-67ed0dbb7c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "positive_density = positivecount + ((differentwords+1)*alpha)\n",
    " \n",
    "positive_train = train.filter(lambda x: x[1] == \"1\").flatMap(lambda x: x[0]).map(lambda x: (x,1)).reduceByKey(lambda x,y:x+y).mapValues(lambda x: (x+alpha)/positive_density) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237158f5-a9a4-4ed4-af31-a4bec9741d72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[95]: [('movie', 0.01818181818181818),\n ('film', 0.010671936758893281),\n ('bad', 0.009881422924901186),\n ('even', 0.005731225296442688),\n ('one', 0.005731225296442688),\n ('plot', 0.0047430830039525695),\n ('like', 0.004545454545454545),\n ('time', 0.004347826086956522),\n ('really', 0.004150197628458498),\n ('acting', 0.004150197628458498),\n ('character', 0.0037549407114624506),\n ('script', 0.0035573122529644267),\n ('scene', 0.0033596837944664033),\n ('make', 0.0031620553359683794),\n ('would', 0.0031620553359683794),\n ('story', 0.0031620553359683794),\n ('line', 0.002766798418972332),\n ('good', 0.002766798418972332),\n ('see', 0.002766798418972332),\n ('work', 0.002569169960474308),\n ('ever', 0.002569169960474308),\n ('seen', 0.002569169960474308),\n ('stupid', 0.002569169960474308),\n ('little', 0.002569169960474308),\n ('much', 0.002569169960474308),\n ('made', 0.002569169960474308),\n ('thing', 0.002569169960474308),\n ('show', 0.002569169960474308),\n ('worst', 0.0023715415019762848),\n ('could', 0.0023715415019762848),\n ('terrible', 0.002173913043478261),\n ('nothing', 0.002173913043478261),\n ('totally', 0.001976284584980237),\n ('look', 0.001976284584980237),\n ('real', 0.001976284584980237),\n ('get', 0.001976284584980237),\n ('suck', 0.001976284584980237),\n ('actor', 0.0017786561264822134),\n ('never', 0.0017786561264822134),\n ('watching', 0.0017786561264822134),\n ('way', 0.0017786561264822134),\n ('worse', 0.0017786561264822134),\n ('also', 0.0017786561264822134),\n ('every', 0.0017786561264822134),\n ('well', 0.0017786561264822134),\n ('piece', 0.0017786561264822134),\n ('minute', 0.0017786561264822134),\n ('waste', 0.0017786561264822134),\n ('awful', 0.0017786561264822134),\n ('watch', 0.0015810276679841897),\n ('predictable', 0.0015810276679841897),\n ('better', 0.0015810276679841897),\n ('dialogue', 0.0015810276679841897),\n ('mess', 0.0015810276679841897),\n ('avoid', 0.0015810276679841897),\n ('boring', 0.001383399209486166),\n ('writing', 0.001383399209486166),\n ('sucked', 0.001383399209486166),\n ('know', 0.001383399209486166),\n ('slow', 0.001383399209486166),\n ('least', 0.001383399209486166),\n ('feeling', 0.001383399209486166),\n ('whole', 0.001383399209486166),\n ('lot', 0.001383399209486166),\n ('lack', 0.0011857707509881424),\n ('crap', 0.0011857707509881424),\n ('say', 0.0011857707509881424),\n ('kind', 0.0011857707509881424),\n ('directing', 0.0011857707509881424),\n ('thought', 0.0011857707509881424),\n ('everything', 0.0011857707509881424),\n ('probably', 0.0011857707509881424),\n ('hole', 0.0011857707509881424),\n ('action', 0.0011857707509881424),\n ('fails', 0.0011857707509881424),\n ('used', 0.0011857707509881424),\n ('year', 0.0011857707509881424),\n ('garbage', 0.0011857707509881424),\n ('hour', 0.0011857707509881424),\n ('part', 0.0011857707509881424),\n ('flick', 0.0011857707509881424),\n ('whatever', 0.0011857707509881424),\n ('cost', 0.0011857707509881424),\n ('horror', 0.0011857707509881424),\n ('director', 0.0011857707509881424),\n ('cheap', 0.0011857707509881424),\n ('first', 0.0011857707509881424),\n ('almost', 0.0011857707509881424),\n ('place', 0.0011857707509881424),\n ('anything', 0.0009881422924901185),\n ('wasted', 0.0009881422924901185),\n ('two', 0.0009881422924901185),\n ('far', 0.0009881422924901185),\n ('best', 0.0009881422924901185),\n ('mediocre', 0.0009881422924901185),\n ('away', 0.0009881422924901185),\n ('annoying', 0.0009881422924901185),\n ('quite', 0.0009881422924901185),\n ('pretentious', 0.0009881422924901185),\n ('poor', 0.0009881422924901185)]"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "negative_density = negativecount + ((differentwords+1)*alpha)\n",
    " \n",
    "negative_train = train.filter(lambda x: x[1] == \"0\").flatMap(lambda x: x[0]).map(lambda x: (x,1)).reduceByKey(lambda x,y:x+y).mapValues(lambda x: (x+alpha)/negative_density) \n",
    "negative_train.sortBy(lambda x: -x[1]).take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57edd24e-0614-493b-aad0-4d00dbfe3ae5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[96]: [('attempting', 0),\n ('artiness', 0),\n ('black', 0),\n ('white', 0),\n ('clever', 0),\n ('camera', 0),\n ('angle', 0),\n ('movie', 0),\n ('disappointed', 0),\n ('became', 0)]"
     ]
    }
   ],
   "source": [
    "test_index = test.zipWithIndex()\n",
    "test_data = test_index.map(lambda x: (x[1],x[0][0]))\n",
    "test_label = test_index.map(lambda x: (x[1],x[0][1]))\n",
    "split_test_words = test_data.flatMap(lambda x:((x[0],(i)) for i in x[1])).map(lambda x: (x[1],x[0]))\n",
    "split_test_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703ba2a3-2a84-4296-afb4-d798dede9ffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "positive_probability_test = split_test_words.leftOuterJoin(positive_train).map(lambda x: (x[0],(x[1][0],1/positive_density if x[1][1] is None else x[1][1])))\n",
    "positive_probability_log_test = positive_probability_test.map(lambda x: (x[1][0],x[1][1])).map(lambda x: (x[0],math.log(x[1]))).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0],x[1]+positive_log_probablity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de1943c-7dc6-40da-9e25-ac6bb1e6e204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "neg_probability_test = split_test_words.leftOuterJoin(negative_train).map(lambda x: (x[0],(x[1][0],1/negative_density if x[1][1] is None else x[1][1])))\n",
    "neg_probability_log_test = neg_probability_test.map(lambda x: (x[1][0],x[1][1])).map(lambda x: (x[0],math.log(x[1]))).reduceByKey(lambda x,y: x+y).map(lambda x: (x[0],x[1]+negative_log_probablity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df0af9c-be31-423f-961f-b6e8421df1de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_probability = neg_probability_log_test.fullOuterJoin(positive_probability_log_test)\n",
    "predict_test = test_probability.map(lambda x: (x[0],\"0\" if x[1][0] > x[1][1] else \"1\"))\n",
    "original_predict = predict_test.join(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eced06ed-1910-4afd-971b-ac7d5ab2baa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "org_count = original_predict.filter(lambda x: x[1][1] == x[1][0]).count() \n",
    "accuracy =(org_count)/(test_count)\n",
    "prior_neg = negatives/ (train_count)\n",
    "prior_pos = positives/ (train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb19015d-2ccc-49b9-9a19-98d466bc3db3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model  0.8052631578947368\nprior probability of positive reveiws  0.5074074074074074\nprior probability of negative reviews  0.4925925925925926\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model \", accuracy)\n",
    "print(\"prior probability of positive reveiws \", prior_pos)\n",
    "print(\"prior probability of negative reviews \", prior_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e67aa9d-ba59-4d81-af3d-3b30d8e0cb80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BigdataAssignment2Part2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
